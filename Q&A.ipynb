{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "mount_file_id": "1m3jP8GCzNavMh99aEF-3qeACaj4qDAMw",
      "authorship_tag": "ABX9TyM01ADyZgLLvEUyQKQTjSAW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chw8207/Transformer/blob/main/Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "C5L51v1BO7TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farm-haystack"
      ],
      "metadata": {
        "id": "UFEiYXaNVkFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install farm-haystack[colab,inference]"
      ],
      "metadata": {
        "id": "Iu4zD7iPaZWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vOljS_ThDwVm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import pipeline, AutoModelForQuestionAnswering, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPU 설정"
      ],
      "metadata": {
        "id": "_uDtMwxDFHvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0' :\n",
        "    raise SystemError('GPU device not found')\n",
        "print(f'Found GPU at: {device_name}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wKa3pEOFGCq",
        "outputId": "257af359-7679-4e27-9f41-5ac8e57b0092"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(device_lib.list_local_devices())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaoOmiQMFHaL",
        "outputId": "c1b84cd7-e417-4c90-dbdf-9c072637a8d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 11420901334832857200\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15208677376\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5765050721822656387\n",
            "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
            "xla_global_id: 416903419\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 방법론"
      ],
      "metadata": {
        "id": "yVb28CVwIhqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 방법0: 시행착오"
      ],
      "metadata": {
        "id": "BOe-jpPIOFYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디폴트 모델과 토크나이저\n",
        "nlp_qa = pipeline('question-answering')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c28gM9BWPfKo",
        "outputId": "32501a49-3ae2-4439-9fe3-0e89156f2e17"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"\"\"The traffic began to slow down on Pioneer Boulevard in Los Angeles,\n",
        "making it difficult to get out of the city. However, WBGO was playing some cool jazz,\n",
        "and the weather was cool, making it rather pleasant to be making it out of the city\n",
        "on this Friday afternoon. Nat King Cole was singing as Jo and Maria slowly made\n",
        "their way out of LA and drove toward Barstow. They planned to get to Las Vegas\n",
        "early enough in the evening to have a nice dinner and go see a show.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "tdmm92UJPunw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa(context=sequence, question='Where is Pioneer Boulevard?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6koaYWxQP8wd",
        "outputId": "394cd6a6-145e-4a27-963f-0574b06b69b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9879735112190247, 'start': 55, 'end': 66, 'answer': 'Los Angeles'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 방법 1: NER firts"
      ],
      "metadata": {
        "id": "Y_FoZAILRTtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 질문을 찾기 위해 NER 사용하기\n",
        "nlp_ner = pipeline('ner')\n",
        "print(nlp_ner(sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6FtxtPxRWit",
        "outputId": "7925fa9e-3225-45e3-ff4d-ff4c14889a5e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity': 'I-LOC', 'score': 0.9735258, 'index': 8, 'word': 'Pioneer', 'start': 34, 'end': 41}, {'entity': 'I-LOC', 'score': 0.9944825, 'index': 9, 'word': 'Boulevard', 'start': 42, 'end': 51}, {'entity': 'I-LOC', 'score': 0.9995776, 'index': 11, 'word': 'Los', 'start': 55, 'end': 58}, {'entity': 'I-LOC', 'score': 0.99956936, 'index': 12, 'word': 'Angeles', 'start': 59, 'end': 66}, {'entity': 'I-ORG', 'score': 0.9919844, 'index': 26, 'word': 'W', 'start': 121, 'end': 122}, {'entity': 'I-ORG', 'score': 0.9907504, 'index': 27, 'word': '##B', 'start': 122, 'end': 123}, {'entity': 'I-ORG', 'score': 0.9884582, 'index': 28, 'word': '##G', 'start': 123, 'end': 124}, {'entity': 'I-ORG', 'score': 0.9722683, 'index': 29, 'word': '##O', 'start': 124, 'end': 125}, {'entity': 'I-PER', 'score': 0.99668807, 'index': 59, 'word': 'Nat', 'start': 264, 'end': 267}, {'entity': 'I-PER', 'score': 0.9976484, 'index': 60, 'word': 'King', 'start': 268, 'end': 272}, {'entity': 'I-PER', 'score': 0.9986173, 'index': 61, 'word': 'Cole', 'start': 273, 'end': 277}, {'entity': 'I-PER', 'score': 0.9978789, 'index': 65, 'word': 'Jo', 'start': 293, 'end': 295}, {'entity': 'I-PER', 'score': 0.9988164, 'index': 67, 'word': 'Maria', 'start': 300, 'end': 305}, {'entity': 'I-LOC', 'score': 0.99813443, 'index': 74, 'word': 'LA', 'start': 335, 'end': 337}, {'entity': 'I-LOC', 'score': 0.99702674, 'index': 78, 'word': 'Bar', 'start': 355, 'end': 358}, {'entity': 'I-LOC', 'score': 0.8573917, 'index': 79, 'word': '##sto', 'start': 358, 'end': 361}, {'entity': 'I-LOC', 'score': 0.992025, 'index': 80, 'word': '##w', 'start': 361, 'end': 362}, {'entity': 'I-LOC', 'score': 0.99935514, 'index': 87, 'word': 'Las', 'start': 387, 'end': 390}, {'entity': 'I-LOC', 'score': 0.99895394, 'index': 88, 'word': 'Vegas', 'start': 391, 'end': 396}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 위치 엔터티 질문\n",
        "nlp_qa = pipeline('question-answering')\n",
        "print('Question 1.', nlp_qa(context=sequence, question='Where is Pioneer Boulevard?'))\n",
        "print('Question 2.', nlp_qa(context=sequence, question='Where is Los Angeles located?'))\n",
        "print('Question 3.', nlp_qa(context=sequence, question='Where is LA?'))\n",
        "print('Question 4.', nlp_qa(context=sequence, question='Where is Barstow?'))\n",
        "print('Question 5.', nlp_qa(context=sequence, question='Where is Las Vegas located?'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VJ_gnIZVyrK",
        "outputId": "4449cf33-102b-4cd9-c46d-815ef2ccef0e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1. {'score': 0.9879735112190247, 'start': 55, 'end': 66, 'answer': 'Los Angeles'}\n",
            "Question 2. {'score': 0.9875389337539673, 'start': 34, 'end': 51, 'answer': 'Pioneer Boulevard'}\n",
            "Question 3. {'score': 0.5090532302856445, 'start': 55, 'end': 66, 'answer': 'Los Angeles'}\n",
            "Question 4. {'score': 0.36954542994499207, 'start': 387, 'end': 396, 'answer': 'Las Vegas'}\n",
            "Question 5. {'score': 0.2183990627527237, 'start': 355, 'end': 362, 'answer': 'Barstow'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 개인 엔터티 질문\n",
        "nlp_qa = pipeline('question-answering')\n",
        "nlp_qa(context=sequence, question='Who was singing?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoytcyCAZv3X",
        "outputId": "919e3217-f962-4582-9c61-6fbd49469783"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.9653673768043518,\n",
              " 'start': 264,\n",
              " 'end': 277,\n",
              " 'answer': 'Nat King Cole'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa(context=sequence, question='Who was going to Las Vegas?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvCSIWF1aHd8",
        "outputId": "d305e41b-d8fa-43b6-8966-1b9d9ed528ef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.4316229820251465,\n",
              " 'start': 264,\n",
              " 'end': 277,\n",
              " 'answer': 'Nat King Cole'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SRL first"
      ],
      "metadata": {
        "id": "8aHEX9yRaNiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa(context=sequence, question='Who are they?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9gYeyljaNZp",
        "outputId": "9e3548c5-d05c-4209-f416-ab385305ed9a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.8486901521682739,\n",
              " 'start': 293,\n",
              " 'end': 305,\n",
              " 'answer': 'Jo and Maria'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa(context=sequence, question='Who drove to Las Vegas?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt48dVaqcBfh",
        "outputId": "0946f874-52be-41fb-a52b-54a2923f208c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.35941487550735474,\n",
              " 'start': 264,\n",
              " 'end': 305,\n",
              " 'answer': 'Nat King Cole was singing as Jo and Maria'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ELECTRA를 사용한 질문-답변"
      ],
      "metadata": {
        "id": "zsJ3XAXFcKuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp_qa.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ_a18EocKna",
        "outputId": "246703b3-81a9-4bb2-a553-2074004c1ae7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertForQuestionAnswering(\n",
            "  (distilbert): DistilBertModel(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): Transformer(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x TransformerBlock(\n",
            "          (attention): MultiHeadSelfAttention(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (ffn): FFN(\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (activation): GELUActivation()\n",
            "          )\n",
            "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa = pipeline('question-answering', model='google/electra-small-generator',\n",
        "                  tokenizer='google/electra-small-generator')\n",
        "nlp_qa(context=sequence, question='Who drove to Las Vegas?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvxgcJ7j3pyx",
        "outputId": "7554993a-c133-4c73-d3d4-116c7b54fcce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForQuestionAnswering were not initialized from the model checkpoint at google/electra-small-generator and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.0003573082503862679,\n",
              " 'start': 206,\n",
              " 'end': 237,\n",
              " 'answer': 'to be making it out of the city'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa = pipeline('question-answering')\n",
        "nlp_qa(context=sequence, question='What was slow?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSGNCC4OAES_",
        "outputId": "7a43cfc4-01d0-4c37-dd2e-8f0e480ce362"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.46530431509017944, 'start': 0, 'end': 11, 'answer': 'The traffic'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa = pipeline('question-answering')\n",
        "nlp_qa(context=sequence, question='What was playing?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RktkL5RLPmdP",
        "outputId": "8cbd53fc-a8a8-4a4e-d402-7f3078b2a6cc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.48992660641670227, 'start': 143, 'end': 152, 'answer': 'cool jazz'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_qa = pipeline('question-answering')\n",
        "nlp_qa(context=sequence, question='Who sees a show?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH9kYQ73SQ5T",
        "outputId": "c6bb139a-4927-4f63-9b57-71c7f1abb04e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.5588222146034241,\n",
              " 'start': 264,\n",
              " 'end': 277,\n",
              " 'answer': 'Nat King Cole'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RoBERTa 모델로 Haystack 탐구하기"
      ],
      "metadata": {
        "id": "LMUD_xNtVbih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머에서 사용\n",
        "model_name = 'deepset/roberta-base-squad2'\n",
        "\n",
        "# 예측 얻기\n",
        "nlp_qa = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
        "QA_input = {\n",
        "    'question': 'Where is Pioneer Boulevard located?',\n",
        "    'context': sequence\n",
        "}\n",
        "res = nlp_qa(QA_input)\n",
        "\n",
        "# 모델 로딩 % 토크나이징\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "-mxZCDV9VbbE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XD5gpasBczqj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}